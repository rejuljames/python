{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import hashlib\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tarfile\n",
    "from urllib import urlretrieve\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import csv\n",
    "import math\n",
    "import codecs\n",
    "import json \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(string):\n",
    "    \"\"\" Split a tweet into tokens.\"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    string = string.lower()\n",
    "    tokens = []\n",
    "    string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    string = re.sub('@[^\\s]+','AT_USER',string)\n",
    "    #Remove additional white spaces\n",
    "    string = re.sub('[\\s]+', ' ', string)\n",
    "    #Replace #word with word\n",
    "    string = re.sub(r'#([^\\s]+)', r'\\1', string)\n",
    "    string = re.sub(r'\\\\x[0-9a-fA-F]+','HEX',string)\n",
    "    string = re.sub(r'[\\x80-\\xff]','HEX',string)\n",
    "    #tokens = string.split()\n",
    "    tokens = string.split()\n",
    "    for i in range(len(tokens)):\n",
    "        if (tokens[i] == 'not') and (i < len(tokens)-2):\n",
    "            tokens[i+1] = ('not_'+tokens[i+1])\n",
    "            tokens[i+2] = ('not_'+tokens[i+2])\n",
    "        elif (tokens[i] == 'not') and (i == len(tokens)-2):\n",
    "            tokens[i+1] = ('not_'+tokens[i+1])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_not(text):\n",
    "    #tokens = re.findall(r'[.?!\\'*\\\"#$%&\\)\\xe9\\(,+-@;`:<=>\\[\\]^\\{\\}]|\\w+', text.lower())\n",
    "    tokens = re.sub('\\W+', ' ', text).lower().split()    \n",
    "    for i in range(len(tokens)):\n",
    "        if (tokens[i] == 'not') and (i < len(tokens)-2):\n",
    "            tokens[i+1] = ('not_'+tokens[i+1])\n",
    "            tokens[i+2] = ('not_'+tokens[i+2])\n",
    "        elif (tokens[i] == 'not') and (i == len(tokens)-2):\n",
    "            tokens[i+1] = ('not_'+tokens[i+1])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization, Model training and Cross Validation Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def do_vectorize(tweets, tokenizer_fn=tokenize_with_not, min_df=1,\n",
    "                 max_df=1., binary=True, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(input='content',encoding='utf-8', decode_error='ignore',tokenizer=tokenizer_fn, min_df=min_df,max_df=max_df, binary=binary, ngram_range= ngram_range ,analyzer=u'word',dtype=int)\n",
    "    #vectorizer = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode',  \n",
    "    #analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "\n",
    "    X = vectorizer.fit_transform([t for t in tweets])\n",
    "    return (X,vectorizer)\n",
    "    \n",
    "def repeatable_random(seed):\n",
    "    hash = str(seed)\n",
    "    while True:\n",
    "        hash = hashlib.md5(hash).digest()\n",
    "        for c in hash:\n",
    "            yield ord(c)\n",
    "\n",
    "def repeatable_shuffle(X, y, tweets,booknames):\n",
    "    r = repeatable_random(42) \n",
    "    indices = sorted(range(X.shape[0]), key=lambda x: next(r))\n",
    "    return X[indices], y[indices], np.array(tweets)[indices], np.array(booknames)[indices]\n",
    "\n",
    "def get_clf():\n",
    "    return LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
    "                             C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                             class_weight=None, random_state=None)\n",
    "\n",
    "def train_model(X,y):\n",
    "    model = get_clf()\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "def do_cross_validation(X, y, n_folds=5, verbose=False):\n",
    "    cv = KFold(len(y), n_folds)    \n",
    "    accuracies = []\n",
    "    fold = 0\n",
    "    #accuracy = 1. * len([1 for tr, pr in zip(y[test_ind], predictions) if tr == pr]) / len(y[test_ind]))\n",
    "    for train_ind, test_ind in cv:  \n",
    "        model = train_model(X[train_ind], y[train_ind])\n",
    "        predictions = model.predict(X[test_ind])\n",
    "        accuracies.append(accuracy_score(y[test_ind], predictions))\n",
    "        if verbose == True:\n",
    "            print ('fold %d accuracy=%.4f' %(fold,accuracy_score(y[test_ind], predictions)))\n",
    "        fold += 1\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return model,avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#instantiate the model with training data and this model is used for enhancing the accuracy\n",
    "\n",
    "def read_train_file():    \n",
    "    file_reader = csv.reader(open('traindata.csv','r'), delimiter=',', quotechar='\"')\n",
    "    tweets = []\n",
    "    for row in file_reader:\n",
    "        sentiment = row[0]\n",
    "        #feature_vector = get_features(tokenize(row[5]))\n",
    "        name_of_book = row[2]\n",
    "        tweets.append({'sentiment':int(sentiment),'text':row[1],'name_of_book':row[2]})\n",
    "        book_names = [n['name_of_book'] for n in tweets]\n",
    "    print 'read %d tweets' % len(tweets)\n",
    "    labels = np.array([t['sentiment'] for t in tweets])\n",
    "    train_tweets = [(t['text']) for t in tweets]\n",
    "    return train_tweets,labels,book_names\n",
    "\n",
    "def train_accuracy(tweets,labels,booknames):\n",
    "    matrix, vec = do_vectorize(tweets,tokenizer_fn=preprocess, min_df=1,max_df=1., binary=True, ngram_range=(1,1))\n",
    "    X, y, tweet,names_of_books = repeatable_shuffle(matrix, labels, tweets,booknames)\n",
    "    print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])\n",
    "    print ('matrix represents %d tweets with %d features' % (matrix.shape[0], matrix.shape[1]))\n",
    "    #X_train, X_test, y_train, y_test,bnames_train,bnames_test = train_test_split(X, y, names_of_books,test_size=0.3,random_state=0)\n",
    "    #train_model=LogisticRegression()\n",
    "    #train_model.fit(X_train, y_train)\n",
    "    #predictions = train_model.predict(X_test) \n",
    "    model, avg_accuracy =  do_cross_validation(X, y, verbose=False)\n",
    "    print('average cross validation accuracy=%.4f' % avg_accuracy)\n",
    "    return vec,model\n",
    "tweets,labels,booknames = read_train_file()\n",
    "vectorizer,model = train_accuracy(tweets,labels,booknames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments, Parameter Tuning to Enhance Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(tweets, y, tokenizer_fn=preprocess,\n",
    "            min_df=1, max_df=1., binary=True,\n",
    "            ngram_range=(1,1), n_folds=5):\n",
    "    matrix, vec = do_vectorize(tweets,tokenizer_fn=tokenizer_fn, min_df=min_df, max_df=max_df, binary=binary, ngram_range=(1,1))\n",
    "    model,avg_accuracy = do_cross_validation(matrix, y,n_folds)\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_binary(tweets, y):    \n",
    "\n",
    "    result_accuracy = []\n",
    "    result_accuracy.append(experiment(tweets,y, binary=True))\n",
    "    result_accuracy.append(experiment(tweets,y, binary=False))\n",
    "    return result_accuracy    \n",
    "          \n",
    "compare_binary(tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer_expt(tweets,y):\n",
    "    \"\"\"\n",
    "    a list of average testing accuracies for each tokenizer.\n",
    "    \"\"\"\n",
    "    r_accuracy = []\n",
    "    r_accuracy.append(experiment(tweets,y, tokenizer_fn=preprocess))\n",
    "    r_accuracy.append(experiment(tweets,y, tokenizer_fn=tokenize_with_not))\n",
    "    return r_accuracy     \n",
    "\n",
    "tokenizer_expt(tweets,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def min_df_expt(tweets, y):\n",
    " \n",
    "    r_accuracy = []\n",
    "    for min_value in range(1,11):\n",
    "        r_accuracy.append(experiment(tweets,y, min_df=min_value, tokenizer_fn=tokenize_with_not))\n",
    "    plt.plot(range(1,11),r_accuracy, 'bo-', label ='min_df accuracy')\n",
    "    plt.xlabel('min_df')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()    \n",
    "    return r_accuracy\n",
    "\n",
    "min_df_expt(tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_df_expt(tweets, y):\n",
    " \n",
    "    r_accuracy = []\n",
    "    max_df_list = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.]\n",
    "    for max_df_val in max_df_list:\n",
    "        r_accuracy.append(experiment(tweets,y, tokenizer_fn=tokenize_with_not, min_df=1, max_df=max_df_val))\n",
    "    plt.plot(range(1,11),r_accuracy, 'bo-', label ='max_df accuracy')\n",
    "    plt.xlabel('max_df')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()     \n",
    "    return r_accuracy        \n",
    "    \n",
    "max_df_expt(tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def enhance_accuracy(tweets,labels,booknames):\n",
    "    matrix, vec = do_vectorize(tweets,tokenizer_fn=tokenize_with_not, min_df=1,max_df=.3, binary=True, ngram_range=(1,1))\n",
    "    X, y, tweet,names_of_books = repeatable_shuffle(matrix, labels, tweets,booknames)\n",
    "    print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])\n",
    "    print ('matrix represents %d tweets with %d features' % (matrix.shape[0], matrix.shape[1]))\n",
    "    model, avg_accuracy = do_cross_validation(X, y, verbose=False)\n",
    "    print('average cross validation accuracy=%.4f' % avg_accuracy)\n",
    "    return vec,model,X, y, tweet,names_of_books\n",
    "#tweets,labels,booknames = read_train_file()\n",
    "vectorizer,model,X,y,tweet,booknames = enhance_accuracy(tweets,labels,booknames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [\"a\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"did\", \"do\", \"does\", \"doing\", \"don\", \"down\", \n",
    "\"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"get\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\",\"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"im\", \"i'm\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"just\", \"me\",\n",
    "\"most\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\",\"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \n",
    "\"out\", \"over\", \"own\", \"rt\", \"s\", \"same\", \"she\", \"should\", \"so\",\"some\", \"such\", \"t\", \"than\", \"that\", \"the\", \"their\", \"theirs\",\"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"this_is_a_url\"\n",
    "\"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"us\",\"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "\"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_1 =[\"pewdiepie\",\"soundless\",\"Nicole\", \"Magnus\",\"bazaar\",\"Reagen\",\"crows\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Book Ratings using Logistic Regression Model (Accuracy = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluatig the model by splitting the data into test and train data\n",
    "\n",
    "def read_test_file():    \n",
    "    file_reader = csv.reader(open('testdata_not_labeled.csv','r'), delimiter=',', quotechar='\"')\n",
    "    #for row in codecs.open('magnus_labeled_new.txt', encoding=\"utf-8\"):\n",
    "    tweets = []        \n",
    "    for row in file_reader:\n",
    "        #row = row.strip().split('\\t')\n",
    "        #text = json.loads(row[1])\n",
    "        #sentiment = row[0]\n",
    "        name_of_book = row[1]\n",
    "        #feature_vector = get_features(tokenize(row[5]))\n",
    "        tweets.append({'text':row[0],'name_of_book':name_of_book})\n",
    "    #labels = np.array([t['sentiment'] for t in tweets])\n",
    "    train_tweets = [(t['text']) for t in tweets]\n",
    "    book_names = [n['name_of_book'] for n in tweets]\n",
    "    return train_tweets,book_names\n",
    "\n",
    "def ratings_book(X_test,booknames,train_model):\n",
    "    ratings = 0.0\n",
    "    predictions = train_model.predict(X_test) \n",
    "    num_books = Counter(booknames)\n",
    "    ratings = {}\n",
    "    for book in num_books.keys():\n",
    "        #b_labels =[y_test[b] for b in range(len(booknames)) if booknames[b] == book]\n",
    "        positives = [predictions[b] for b in range(len(booknames)) if (predictions[b] == 4) and (booknames[b] == book)]\n",
    "        total_tweets = [predictions[b] for b in range(len(booknames)) if booknames[b] == book]\n",
    "        neutrals = [predictions[b] for b in range(len(booknames)) if (predictions[b] == 2) and (booknames[b] == book)]\n",
    "        b_rating = float(len(positives))/float(len(total_tweets)-len(neutrals))\n",
    "        ratings[book] = b_rating*5\n",
    "    return ratings\n",
    "\n",
    "def plotting(booknames,ratings):\n",
    "    print booknames\n",
    "    print ratings\n",
    "    #plt.rcdefaults()\n",
    "    y_pos = np.arange(len(ratings))\n",
    "    plt.figure(figsize = (12,10))\n",
    "    plt.bar(y_pos, ratings, width =0.5,align='center', alpha=0.5)\n",
    "    plt.rc('xtick',labelsize = 8)\n",
    "    plt.xticks(y_pos, booknames)\n",
    "    plt.ylabel('Ratings')\n",
    "    plt.title('Book ratings predicted using twitter feeds')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def compare_ratings(book_ratings):\n",
    "    range_ratings = [1.0,2.0,3.0,4.0,5.0,6.0,7.0]    \n",
    "    amazon_ratings = [4.6,4.7,4.2,4.42,4.3,3.7,4.8]\n",
    "    goodread_ratings = [4.03,4.25,4.02,4.03,4.06,3.5,4.25]\n",
    "    booknames=['Future Crimes', 'Magnus Chase','The Bazaar of Bad Dreams','Six of crows', 'Killing Reagen','Soundless','Becoming Nicole']\n",
    "    plt.figure(figsize = (12,10))\n",
    "    plt.rc('xtick',labelsize = 8)\n",
    "    plt.xticks(range_ratings, booknames)\n",
    "    plt.plot(range_ratings, amazon_ratings, marker='o',label='amazon_ratings')\n",
    "    plt.plot(range_ratings, goodread_ratings, marker='o', linestyle='--', color='r', label='goodread_ratings')\n",
    "    plt.plot(range_ratings, book_ratings, marker='o',label='twitter_ratings')\n",
    "    plt.xlabel('Books')\n",
    "    plt.ylabel('Twitter Vs Amazon/GoodReads')\n",
    "    plt.title('Compare Ratings')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def wrapper(t_tweets,b_booknames,model): \n",
    "    X_test = vectorizer.transform(t_tweets)\n",
    "    booknames = []\n",
    "    book_ratings = []\n",
    "    ratings = ratings_book(X_test,b_booknames,model)\n",
    "    print \"Ratings of the books are\\n\"\n",
    "    for key in ratings.keys():\n",
    "        if(ratings[key]>=4.0):\n",
    "            tag = \"Excellent Reviews\"\n",
    "        elif(ratings[key]>=3.0 and ratings[key]<4.0):\n",
    "            tag = \"Good reviews\"\n",
    "        elif(ratings[key]>=2.5 and ratings[key]<3.0):\n",
    "            tag = \"Average reviews\"\n",
    "        elif(ratings[key]<2.5):\n",
    "            tag = \"Bad reviews\"\n",
    "        print key,':\\t',tag,':\\t',ratings[key],'\\n'\n",
    "    for keys in ratings.keys():   \n",
    "        booknames.append(keys)\n",
    "        book_ratings.append(ratings[keys])\n",
    "    plotting(tuple(booknames),book_ratings)\n",
    "    compare_ratings(book_ratings)\n",
    "    return X_test\n",
    "\n",
    "\n",
    "t_tweets, b_booknames = read_test_file()\n",
    "X_test = wrapper(t_tweets,b_booknames,model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - Precision, Recall, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "def eval_metrics(X,y,model):  \n",
    "    predictions = model.predict(X)\n",
    "    print metrics.confusion_matrix(y, predictions)\n",
    "    print metrics.classification_report(y, predictions)\n",
    "    probs = model.predict_proba(X)\n",
    "    print probs\n",
    "eval_metrics(X,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier (Accuracy = 0.645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NaiveBayes\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "\n",
    "def data_preprocess(string):\n",
    "    \"\"\" Split a tweet into tokens.\"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    string = string.lower()\n",
    "    tokens = []\n",
    "    string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    string = re.sub('@[^\\s]+','AT_USER',string)\n",
    "    #Remove additional white spaces\n",
    "    string = re.sub('[\\s]+', ' ', string)\n",
    "    #Replace #word with word\n",
    "    string = re.sub(r'#([^\\s]+)', r'\\1', string)\n",
    "    string = re.sub(r'\\\\x[0-9a-fA-F]+','HEX',string)\n",
    "    string = re.sub(r'[\\x80-\\xff]','HEX',string)\n",
    "    #tokens = string.split()\n",
    "    #tokens = string.split()\n",
    "    return string\n",
    "\n",
    "\n",
    "\n",
    "def get_features(tweet,stopwords):\n",
    "    #stop = nltk.corpus.stopwords.words('english')\n",
    "    feature_vector = []\n",
    "    for word in tweet:\n",
    "        word = replaceTwoOrMore(word)\n",
    "        if word not in stopwords:\n",
    "            feature_vector.append(word.lower())\n",
    "    return feature_vector\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "#def get_words_in_test_tweets(tweets):\n",
    "#    all_words = []\n",
    "#    for words in tweets:\n",
    "#        all_words.append(words)\n",
    "#    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def read_file():  \n",
    "    file_reader = csv.reader(open('magnus_labeled.csv','r'), delimiter=',', quotechar='\"')\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for row in file_reader:\n",
    "        sentiment = row[0]\n",
    "        if row[0] == '4':\n",
    "            sentiment = 'positive' \n",
    "        if row[0] == '0':\n",
    "            sentiment = 'negative'\n",
    "        if row[0] == '2':\n",
    "            sentiment = 'neutral'\n",
    "        labels.append(sentiment)\n",
    "        feature_vector = get_features(data_preprocess(row[1]),stopwords)\n",
    "        tweets.append((feature_vector,sentiment))\n",
    "    return tweets,labels\n",
    "\n",
    "def read_test_file():    \n",
    "    file_reader = csv.reader(open('traindata.csv','r'), delimiter=',', quotechar='\"')\n",
    "    tweets = []\n",
    "    book_names = []\n",
    "    labels = []\n",
    "    for row in file_reader:\n",
    "        if row[0] == '4':\n",
    "            sentiment = 'positive' \n",
    "        if row[0] == '0':\n",
    "            sentiment = 'negative'\n",
    "        if row[0] == '2':\n",
    "            sentiment = 'neutral'\n",
    "        labels.append(sentiment)\n",
    "        feature_vector = get_features(data_preprocess(row[1]),stopwords)\n",
    "        tweets.append((feature_vector,sentiment))\n",
    "        book_names.append(row[1])\n",
    "    return tweets,labels,book_names\n",
    "\n",
    "def extract_features(tweets):\n",
    "    tweet_words = set(tweets)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "def predict(classifier,test_tweets):\n",
    "    predictions =[]\n",
    "    for (tweet,sentiment) in test_tweets:\n",
    "        predictions.append(classifier.classify(extract_features(tweet)))\n",
    "    return predictions\n",
    "\n",
    "def train( tweets):\n",
    "    #tweets = read_file()\n",
    "    #word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "    training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    return classifier                \n",
    "\n",
    "tweets,labels = read_file()\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "classifier = train(tweets)\n",
    "test_tweets,labels_test,booknames = read_test_file()\n",
    "\n",
    "predictions = predict(classifier,test_tweets)\n",
    "print \"Accuracy is \",metrics.accuracy_score(labels_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Geo-plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Function to import a JSON file which contains a list of place names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def world_place_list():\n",
    "    import json\n",
    "    from pprint import pprint\n",
    "    with open('cities_duplicate.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting user location from twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_location(filename):\n",
    "    import re\n",
    "    fo = open(filename, \"r\")\n",
    "    k = []\n",
    "    p = []\n",
    "    t = []\n",
    "    l = []\n",
    "    q = []\n",
    "    r = []\n",
    "    u = []\n",
    "    for line in fo:\n",
    "        k =  line.split('\\t')\n",
    "        if len(k) > 4:\n",
    "            p = k[4]\n",
    "        t = p.split(\":\")\n",
    "        l.append(t[1])\n",
    "    \n",
    "    for i in l:\n",
    "        q = re.sub('[^A-Za-z,]+', '', i)\n",
    "        r = q.split(',')\n",
    "        if len(r[0]) > 2:\n",
    "            u.append(r[0])\n",
    "#        print r[0]\n",
    "        \n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing famous country names from a file and appending to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample():\n",
    "    test = open(\"countries_list.txt\", \"r\")\n",
    "    Country_list = []\n",
    "    for i in test:\n",
    "        Country_list.append(i.strip())\n",
    "    test.close()\n",
    "    return Country_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a file that contain place name of famous fifty countries and appending those place to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def place_list_partial(Country_list,data):\n",
    "    import codecs\n",
    "    import re\n",
    "    place_list = []\n",
    "\n",
    "    f = open('place_list.txt','w')\n",
    "    for i in Country_list:\n",
    "        for j in data[i]:\n",
    "            if len(j) < 10:\n",
    "                place = re.sub('[^A-Za-z,]+', '', j)\n",
    "                place_list.append(place)\n",
    "    \n",
    "    return place_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending a country names to place list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def place_list(place_list):\n",
    "    fo = open(\"countries_list.txt\", \"r\")\n",
    "    for i in fo:\n",
    "        place_list.append(i)\n",
    "    fo.close()   \n",
    "    return place_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleansing the user locations by comparing it with place name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleansed(place_list,u):\n",
    "    Book_user_location = []\n",
    "    for place in place_list:\n",
    "        if place in u:\n",
    "            Book_user_location.append(place)\n",
    "    return Book_user_location\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To generate random number for getting random place names from user location list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random(Book_user_location):\n",
    "    random_number_list = []\n",
    "    import random\n",
    "    for i in range(18):\n",
    "        random_number_list.append(random.randint(0, len(Book_user_location)-1))\n",
    "    print random_number_list\n",
    "    return random_number_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get location names to be plotted in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_location(random_number_list,Book_user_location):\n",
    "    final_location = []\n",
    "    for i in range(len(random_number_list)):\n",
    "        final_location.append(Book_user_location[random_number_list[i]])\n",
    "    return final_location   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get cordinates for a loction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coordinates(data,final_location):\n",
    "    import urllib2\n",
    "    import json\n",
    "    #list1 = ['Chicago', 'England', 'Singapore', 'Perth','Texas','California','Cairo'];\n",
    "    k= []\n",
    "    for i in final_location:\n",
    "#        print i\n",
    "        url = \"https://maps.googleapis.com/maps/api/place/autocomplete/json?input=%22%22\"+i+\"%22%22&types=geocode&key=AIzaSyB6EYbbsl0qtbW6v3ADSKUSl1xw_tRw2vo\"\n",
    "        response = urllib2.urlopen(url)\n",
    "#        print response\n",
    "        d = json.load(response)\n",
    "#        print d\n",
    "        #print i\n",
    "        f =  d['predictions'][0]\n",
    "        l = f[\"place_id\"]\n",
    "#        print l\n",
    "        url = \"https://maps.googleapis.com/maps/api/place/details/json?placeid=\"+l+\"&key=AIzaSyCB-_SxkdLns1SASrNDYyRRX7vQxo3GmnE\"\n",
    "        response = urllib2.urlopen(url)\n",
    "#        print response\n",
    "        data = json.load(response)\n",
    "#        print i\n",
    "        k.append(data['result']['geometry']['location'])\n",
    "#        print k\n",
    "    \n",
    "    return k\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate URL for Google Static Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapplotting(key,place_dict,l,book):\n",
    "    static = \"http://maps.google.com/maps/api/staticmap?size=512x512&maptype=roadmap\"\n",
    "    for i in range(len(place_dict)):\n",
    "        if key == book[0]:\n",
    "#            print \"hi\"\n",
    "            r = place_dict[i]\n",
    "#            print r \n",
    "            lng =  int(r[\"lng\"])\n",
    "            lat =  int(r[\"lat\"])\n",
    "            #print  lat,lng              \n",
    "            l = l + \"&markers=color:red|\" + str(lat)+\",\"+ str(lng)\n",
    "        if key == book[1]:\n",
    "#            print \"hi\"\n",
    "            r = place_dict[i]\n",
    "#            print r \n",
    "            lng =  int(r[\"lng\"])\n",
    "            lat =  int(r[\"lat\"])\n",
    "            #print  lat,lng              \n",
    "            l = l + \"&markers=color:blue|\" + str(lat)+\",\"+ str(lng)\n",
    "        if key == book[2]:\n",
    "#            print \"hi\"\n",
    "            r = place_dict[i]\n",
    "#            print r \n",
    "            lng =  int(r[\"lng\"])\n",
    "            lat =  int(r[\"lat\"])\n",
    "            #print  lat,lng              \n",
    "            l = l + \"&markers=color:black|\" + str(lat)+\",\"+ str(lng)\n",
    "        if key == book[3]:\n",
    "#            print \"hi\"\n",
    "            r = place_dict[i]\n",
    "            #print r \n",
    "            lng =  int(r[\"lng\"])\n",
    "            lat =  int(r[\"lat\"])\n",
    "            #print  lat,lng              \n",
    "            l = l + \"&markers=color:yellow|\" + str(lat)+\",\"+ str(lng)\n",
    "   \n",
    "    return static,l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to parse a cordintates values to Mapplotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(coord,l,book):\n",
    "    b_list =  coord.keys()\n",
    "    for key in coord.keys():\n",
    "        coordinates = coord[key]\n",
    "#        print coordinates\n",
    "        static, l = mapplotting(key,coordinates,l,book)\n",
    "#   print l\n",
    "    return static,l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to open the genearated URL in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def browser(url):\n",
    "    import webbrowser\n",
    "#    print k\n",
    "#    print len(k)\n",
    "#    print k[1]\n",
    "    webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper function to invoke all function in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wrapper():\n",
    "   \n",
    "    data = world_place_list()\n",
    "# magnus chase  furture crimes Six crows being nicole  \n",
    "    book = ['MC.txt.','FN.txt','SC.txt','BN.txt']\n",
    "    Name_Cordinates = {}\n",
    "    l = \" \"\n",
    "    for i in range(len(book)):\n",
    "        u = user_location(book[i])\n",
    "#        print u\n",
    "#    print u\n",
    "        Country_list = sample()\n",
    "#    print Country_list\n",
    "        rejul = place_list_partial(Country_list,data)\n",
    "#    print len(rejul)\n",
    "#    print rejul\n",
    "        james =place_list(rejul)\n",
    "#    print len(james)\n",
    "        Book_user_location = cleansed(james,u)\n",
    "#    print Book_user_location\n",
    "        mary = []\n",
    "        mary = random(Book_user_location)\n",
    "#    print mary\n",
    "        loc = final_location(mary,Book_user_location)\n",
    "#    print loc\n",
    "#    print loc\n",
    "        #from collections import defaultdict\n",
    "        #Name_Cordinates = defaultdict(list)        \n",
    "        points = coordinates(data,loc)\n",
    "        #print book[i]\n",
    "        Name_Cordinates[book[i]]=points\n",
    "        \n",
    "#    print Name_Cordinates\n",
    "    static,l = test(Name_Cordinates,l,book)\n",
    "#    print l\n",
    "#    print Name_Cordinates\n",
    "    url = static + l\n",
    "        \n",
    "#   print points\n",
    "#    static,l = mapplotting(Name_Cordinates)\n",
    "#    print url\n",
    "#    print len(url)\n",
    "    browser(url)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "wrapper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
